Requirement: Model supports this method of running. You see the option available.

Deploy model with Inference Endpoints:
![[Pasted image 20250219182953.png]]

> [!note] First time running? Then authorize:
> ![[Pasted image 20250219183121.png]]

It'll let you choose the compute resource and also informs you of the pricing (here it's 3 cents an hour)
![[Pasted image 20250219183103.png]]


Deploying a new Endpoint can take up to several minutes, depending on the model size.

> [!note] First time deploying a model?
> Allow **browser notifications** for alerts when your Endpoints are ready!
> ![[Pasted image 20250219183300.png]]

If it runs successfully proceed. If it fails to run, refer to troubleshooting docs:
